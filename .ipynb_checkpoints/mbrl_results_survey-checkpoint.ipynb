{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Moral Poll Responses - Responses_final_version.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e2f58855289d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the CSV file without using the first row as header (header=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Moral Poll Responses - Responses_final_version.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define new, meaningful column names for the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_column_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Timestamp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'participant_ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Consent'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'question_ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Moral Judgment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Gender'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Nationality'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Group'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Moral Poll Responses - Responses_final_version.csv'"
     ]
    }
   ],
   "source": [
    "# Load the CSV file without using the first row as header (header=None)\n",
    "df = pd.read_csv('Moral Poll Responses - Responses_final_version.csv', header=None)\n",
    "\n",
    "# Define new, meaningful column names for the dataset\n",
    "new_column_names = ['Timestamp', 'participant_ID', 'Consent', 'question_ID', 'Moral Judgment', 'Age', 'Gender', 'Nationality', 'Group']\n",
    "\n",
    "# Assign these new column names to the DataFrame\n",
    "df.columns = new_column_names\n",
    "\n",
    "# Display the first rows of the DataFrame with the updated column names\n",
    "print(df.head())\n",
    "\n",
    "# Step 1: Identify participants who answered exactly 30 questions (complete responders)\n",
    "counts = df['participant_ID'].value_counts()\n",
    "valid_participants = counts[counts == 30].index\n",
    "\n",
    "# Step 2: Filter the DataFrame to keep only data from these valid participants\n",
    "df2 = df[df['participant_ID'].isin(valid_participants)]\n",
    "\n",
    "# Step 3: Print the number of participants with exactly 30 responses\n",
    "print(f\"Number of participants with exactly 30 responses: {len(valid_participants)}\")\n",
    "\n",
    "# Remove duplicate rows to keep only one record per participant (for demographic data)\n",
    "df_unique = df2.drop_duplicates(subset='participant_ID')\n",
    "\n",
    "# Count the number of participants per nationality\n",
    "nationality_counts = df_unique['Nationality'].value_counts()\n",
    "\n",
    "# Calculate the mean and standard deviation of participants' ages\n",
    "mean_age = df_unique['Age'].mean()\n",
    "std_age = df_unique['Age'].std()\n",
    "\n",
    "# Print nationality counts and age statistics\n",
    "print(\"Number of participants per nationality:\")\n",
    "print(nationality_counts)\n",
    "print(f\"\\nMean age: {mean_age:.2f}\")\n",
    "print(f\"Age standard deviation: {std_age:.2f}\")\n",
    "\n",
    "# Calculate the proportion of each gender within the filtered participants (divided by 30 questions per participant)\n",
    "gender_counts = df2['Gender'].value_counts() / 30\n",
    "print(\"\\nProportion of responses by gender:\")\n",
    "print(gender_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of Moral Judgment Distributions Across Scenarios and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a mapping to convert categorical moral judgments to numerical values\n",
    "judgment_map = {\n",
    "    'Support': 1,\n",
    "    'Neutral': 0,\n",
    "    'Blame': -1\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store processed data dictionaries\n",
    "modified_result_list = []\n",
    "\n",
    "# Group the filtered DataFrame by question_ID and process each group\n",
    "for _id, group in df2.groupby('question_ID'):\n",
    "    # Extract the action code from the first 3 characters of question_ID\n",
    "    action = _id[:3]\n",
    "    \n",
    "    # Map 'Moral Judgment' text to numerical values using the defined mapping\n",
    "    reward = group['Moral Judgment'].map(judgment_map).tolist()\n",
    "    \n",
    "    # The state corresponds to the full question_ID\n",
    "    states = _id\n",
    "    \n",
    "    # Create a dictionary for each question with action, reward list, and state\n",
    "    result_dict = {'Action': action, 'Reward': reward, 'State': states}\n",
    "    \n",
    "    # Append this dictionary to the list\n",
    "    modified_result_list.append(result_dict)\n",
    "\n",
    "# A dictionary to translate action codes into descriptive action names\n",
    "action_dict = {\n",
    "    'K_1': 'To perform euthanasia',\n",
    "    'K_2': 'To kill to protect',\n",
    "    'D_1': 'To lie to support',\n",
    "    'D_2': 'To lie by interest',\n",
    "    'L_1': 'To steal',\n",
    "    'L_2': 'To participate in an illegal protest'\n",
    "}\n",
    "\n",
    "\n",
    "# Set academic style for plots\n",
    "sns.set(style=\"whitegrid\", context=\"paper\", font_scale=1.2)\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "# Define a base color palette for actions (from seaborn \"Paired\" palette)\n",
    "palette = sns.color_palette(\"Paired\", 6)\n",
    "base_colors = {\n",
    "    'K_1': palette[0],\n",
    "    'K_2': palette[1],\n",
    "    'D_1': palette[2],\n",
    "    'D_2': palette[3],\n",
    "    'L_1': palette[4],\n",
    "    'L_2': palette[5]\n",
    "}\n",
    "\n",
    "# Convert the list of dictionaries into a proper DataFrame for plotting\n",
    "rows = []\n",
    "for entry in modified_result_list:\n",
    "    action = entry[\"Action\"]\n",
    "    state = entry[\"State\"]\n",
    "    for reward in entry[\"Reward\"]:\n",
    "        rows.append({\"Action\": action, \"State\": state, \"Reward\": reward})\n",
    "\n",
    "df_proper = pd.DataFrame(rows)\n",
    "\n",
    "# Extract the unique actions for plotting\n",
    "actions = df_proper[\"Action\"].unique()\n",
    "\n",
    "# Loop through each action to plot distributions of moral judgments by state\n",
    "for action in actions:\n",
    "    action_df = df_proper[df_proper[\"Action\"] == action]\n",
    "    states = natsorted(action_df[\"State\"].unique())\n",
    "\n",
    "    n_states = len(states)\n",
    "    n_cols = min(3, n_states)  # max 3 columns per figure\n",
    "    n_rows = (n_states + n_cols - 1) // n_cols\n",
    "\n",
    "    # Create subplot grid with constrained layout\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 3.5 * n_rows), constrained_layout=True)\n",
    "\n",
    "    # Ensure axes is a flat array even if there is only one subplot\n",
    "    if n_states == 1:\n",
    "        axes = np.array([axes])\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    # Generate a light palette of colors based on the base color for this action\n",
    "    base_color_name = base_colors.get(action, 'gray')\n",
    "    palette = sns.light_palette(base_color_name, n_states, reverse=False)\n",
    "\n",
    "    # Plot histograms of moral judgment distributions for each state\n",
    "    for ax, state, color in zip(axes, states, palette):\n",
    "        rewards = action_df[action_df[\"State\"] == state][\"Reward\"]\n",
    "        sns.histplot(\n",
    "            rewards,\n",
    "            bins=np.arange(-1.5, 2.5, 1),  # bins centered at -1, 0, 1\n",
    "            ax=ax,\n",
    "            color=color,\n",
    "            edgecolor=\"black\",\n",
    "            alpha=0.9,\n",
    "            kde=False,\n",
    "            stat=\"density\"\n",
    "        )\n",
    "        ax.set_title(f\"State: {state}\", fontsize=12, weight='semibold')\n",
    "        ax.set_xlabel(\"Moral Judgment\", fontsize=11)\n",
    "        ax.set_ylabel(\"Density\", fontsize=11)\n",
    "        ax.set_xticks([-1, 0, 1])\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "    # Hide any unused subplot axes\n",
    "    for ax in axes[n_states:]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    # Add a main title with the descriptive action name\n",
    "    fig.suptitle(f\"Distribution of Judgments for Action: {action_dict[action]}\", fontsize=15, fontweight=\"bold\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add an outcome if none "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniformisation(reward_distrib,n):\n",
    "    if reward_distrib.count(1) == 0:\n",
    "        reward_distrib.append(1)\n",
    "        n+=1\n",
    "    if reward_distrib.count(0)== 0:\n",
    "        reward_distrib.append(0)\n",
    "        n+=1\n",
    "    if reward_distrib.count(-1)==0:\n",
    "        reward_distrib.append(-1)\n",
    "        n+=1\n",
    "    return reward_distrib,n\n",
    "\n",
    "modified_result_list_2 = []\n",
    "n=0\n",
    "for i in range(len(modified_result_list)):\n",
    "    interm = {}\n",
    "    interm['Action']=modified_result_list[i]['Action']\n",
    "    double_output = uniformisation(modified_result_list[i]['Reward'],n)\n",
    "    interm['Reward'],n = double_output[0],double_output[1]\n",
    "    interm['State']= modified_result_list[i]['State']\n",
    "    modified_result_list_2.append(interm)\n",
    "print('Number of outcome modified (+1):', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the MBRL Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to compute the Kullback-Leibler (KL) divergence\n",
    "def DKL(d1, d2):\n",
    "    \"\"\"\n",
    "    Compute the Kullback-Leibler (KL) divergence between two distributions.\n",
    "    \n",
    "    Arguments:\n",
    "    - d1: First probability distribution (list or array)\n",
    "    - d2: Second probability distribution (list or array)\n",
    "\n",
    "    Returns:\n",
    "    - KL divergence value\n",
    "    \"\"\"\n",
    "    if not isinstance(d1, np.ndarray) and not isinstance(d2, np.ndarray):\n",
    "        d1_array = np.array([d1[-1], d1[0], d1[1]])\n",
    "        d2_array = np.array([d2[-1], d2[0], d2[1]])\n",
    "        return np.sum(d1_array * np.log(d1_array / d2_array))\n",
    "    \n",
    "    elif isinstance(d1, np.ndarray) and not isinstance(d2, np.ndarray):\n",
    "        d2_array = np.array([d2[-1], d2[0], d2[1]])\n",
    "        return np.sum(d1 * np.log(d1 / d2_array))\n",
    "    \n",
    "    elif isinstance(d2, np.ndarray) and not isinstance(d1, np.ndarray):\n",
    "        d1_array = np.array([d1[-1], d1[0], d1[1]])\n",
    "        return np.sum(d1_array * np.log(d1_array / d2)) \n",
    "    \n",
    "    else:\n",
    "        return np.sum(d1 * np.log(d1 / d2))\n",
    "\n",
    "# Function to compute the symmetrized Jensen-Shannon (JS) divergence\n",
    "def swJS(d1, d2, N1, N2):\n",
    "    \"\"\"\n",
    "    Compute the symmetrized Jensen-Shannon divergence between two distributions.\n",
    "\n",
    "    Arguments:\n",
    "    - d1: First probability distribution\n",
    "    - d2: Second probability distribution\n",
    "    - N1: Weight of the first distribution\n",
    "    - N2: Weight of the second distribution\n",
    "\n",
    "    Returns:\n",
    "    - Symmetrized JS divergence value\n",
    "    \"\"\"\n",
    "    d1_array = np.array([d1[-1], d1[0], d1[1]])\n",
    "    d2_array = np.array([d2[-1], d2[0], d2[1]])\n",
    "    \n",
    "    # Compute the mixture distribution\n",
    "    mix = (N1 * d1_array + N2 * d2_array) / (N1 + N2)\n",
    "    \n",
    "    return 0.5 * DKL(d1_array, mix) + 0.5 * DKL(d2_array, mix)\n",
    "\n",
    "# Function to estimate a probability distribution from a sample\n",
    "def distribution(samples):\n",
    "    \"\"\"\n",
    "    Compute a probability distribution from a list of samples.\n",
    "\n",
    "    Arguments:\n",
    "    - samples: List of observed values (-1, 0, or 1)\n",
    "\n",
    "    Returns:\n",
    "    - Normalized probability distribution dictionary\n",
    "    \"\"\"\n",
    "    unique, counts = np.unique(samples, return_counts=True)\n",
    "    total = len(samples)\n",
    "\n",
    "    # Compute probabilities\n",
    "    dist = {val: count / total for val, count in zip(unique, counts)}\n",
    "\n",
    "    # Ensure all possible values (-1, 0, 1) have a minimum probability\n",
    "    for val in [-1, 0, 1]:\n",
    "        if val not in dist:\n",
    "            dist[val] = 1e-5\n",
    "\n",
    "    # Normalize probabilities\n",
    "    total_prob = sum(dist.values())\n",
    "    dist = {key: value / total_prob for key, value in dist.items()}\n",
    "\n",
    "    return dist\n",
    "\n",
    "\n",
    "def del_of_a_list(L1,L2):\n",
    "    for val in L2:\n",
    "        if val in L1:\n",
    "            L1.remove(val)\n",
    "\n",
    "\n",
    "\n",
    "def MBRL_agent(dilemma_list, contexts_created, list_of_actions, adding_threshold, merging_threshold):\n",
    "    \"\"\"\n",
    "    Main function to run the MBRL agent.\n",
    "\n",
    "    Arguments:\n",
    "    - dilemma_list: List of dilemmas to process\n",
    "    - Contexts: Dictionary of contexts\n",
    "    - list_of_actions: List of known actions\n",
    "    - adding_threshold: Threshold for adding new contexts\n",
    "    - merging_threshold: Threshold for merging contexts\n",
    "\n",
    "    Returns:\n",
    "    - Updated Contexts dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    for dilemma in dilemma_list:\n",
    "        \"\"\"\n",
    "        Process a dilemma and update existing contexts or create a new one if necessary.\n",
    "\n",
    "        Arguments:\n",
    "        - dilemma: Dictionary containing 'Reward', 'Action', and 'State' values\n",
    "        - Contexts: Dictionary of existing contexts\n",
    "        - list_of_actions: List of actions considered\n",
    "        - threshold: Threshold for KL divergence to determine new contexts\n",
    "        \"\"\"\n",
    "\n",
    "        reward_distribution = distribution(dilemma['Reward'])  \n",
    "\n",
    "        # Check if the action exists in the list\n",
    "        if dilemma['Action'] in list_of_actions:\n",
    "            context_list = list(contexts_created[dilemma['Action']].keys())\n",
    "            DKL_context = []\n",
    "            \n",
    "            \"\"\"\n",
    "            Check if the state is already in memory in a context\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "            for i in range(len(context_list)):\n",
    "                for j in range(len(contexts_created[dilemma['Action']]['C'+str(i+1)]['States'])):\n",
    "                    if dilemma['State'] == contexts_created[dilemma['Action']]['C'+str(i+1)]['States'][j][0]:\n",
    "                        dilemma['Reward'].extend(contexts_created[dilemma['Action']]['C'+str(i+1)]['States'][j][1])\n",
    "                        del_of_a_list(contexts_created[dilemma['Action']]['C'+str(i+1)]['Outcomes'],contexts_created[dilemma['Action']]['C'+str(i+1)]['States'][j][1])\n",
    "                        del contexts_created[dilemma['Action']]['C'+str(i+1)]['States'][j]\n",
    "                        contexts_created[dilemma['Action']]['C'+str(i+1)]['Distribution'] = distribution(\n",
    "                            contexts_created[dilemma['Action']]['C'+str(i+1)]['Outcomes'])\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "            # Compute KL divergence for each context\n",
    "            for i in range(len(context_list)):\n",
    "                DKL_context.append(DKL(reward_distribution, contexts_created[dilemma['Action']]['C'+str(i+1)]['Distribution']))\n",
    "\n",
    "            min_dkl, context_min_dkl = np.min(DKL_context), np.argmin(DKL_context)\n",
    "\n",
    "            if min_dkl > adding_threshold:\n",
    "                # Create a new context if KL divergence exceeds the threshold\n",
    "                contexts_created[dilemma['Action']]['C'+str(1+len(contexts_created[dilemma['Action']]))] = {\n",
    "                    'Distribution': reward_distribution,\n",
    "                    'Outcomes': dilemma['Reward'],\n",
    "                    'States': [[dilemma['State'], dilemma['Reward']]],\n",
    "                }\n",
    "            else:\n",
    "                # Update existing context\n",
    "                contexts_created[dilemma['Action']]['C'+str(context_min_dkl+1)]['Outcomes'] = np.concatenate(\n",
    "                    (contexts_created[dilemma['Action']]['C'+str(context_min_dkl+1)]['Outcomes'], dilemma['Reward']))\n",
    "                contexts_created[dilemma['Action']]['C'+str(context_min_dkl+1)]['States'].append([dilemma['State'], dilemma['Reward']])\n",
    "                contexts_created[dilemma['Action']]['C'+str(context_min_dkl+1)]['Distribution'] = distribution(\n",
    "                    contexts_created[dilemma['Action']]['C'+str(context_min_dkl+1)]['Outcomes'])\n",
    "\n",
    "        else:\n",
    "            # If action is new, add it and create a new context\n",
    "            list_of_actions.append(dilemma['Action'])\n",
    "            contexts_created[dilemma['Action']] = {\n",
    "                'C1': {\n",
    "                    'Distribution': reward_distribution,\n",
    "                    'Outcomes': dilemma['Reward'],\n",
    "                    'States': [[dilemma['State'], dilemma['Reward']]],\n",
    "                }\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Merge similar contexts based on JS divergence.\n",
    "\n",
    "        Arguments:\n",
    "        - Contexts: Dictionary of contexts\n",
    "        - threshold: Merging threshold\n",
    "        \"\"\"\n",
    "        action_list = list(contexts_created.keys())\n",
    "\n",
    "        for action in action_list:\n",
    "            swJS_list = [[], []]\n",
    "\n",
    "            if len(contexts_created[action]) > 1:\n",
    "                for i in range(len(contexts_created[action])):\n",
    "                    for j in range(i+1, len(contexts_created[action])):\n",
    "                        swJS_list[0].append([i, j])\n",
    "                        swJS_list[1].append(\n",
    "                            swJS(contexts_created[action]['C'+str(i+1)]['Distribution'],\n",
    "                                 contexts_created[action]['C'+str(j+1)]['Distribution'],\n",
    "                                 len(contexts_created[action]['C'+str(i+1)]['Outcomes']),\n",
    "                                 len(contexts_created[action]['C'+str(j+1)]['Outcomes']))\n",
    "                        )\n",
    "\n",
    "                min_swJS, context_min_swJS = np.min(swJS_list[1]), np.argmin(swJS_list[1])\n",
    "    \n",
    "                if min_swJS < merging_threshold:\n",
    "                    i, j = swJS_list[0][context_min_swJS]\n",
    "                    n = len(contexts_created[action])\n",
    "                    contexts_created[action]['C'+str(i+1)]['Outcomes'] = list(contexts_created[action]['C'+str(i+1)]['Outcomes'])\n",
    "                    contexts_created[action]['C'+str(j+1)]['Outcomes'] = list(contexts_created[action]['C'+str(j+1)]['Outcomes'])\n",
    "                    contexts_created[action]['C'+str(i+1)]['States'] = list(contexts_created[action]['C'+str(i+1)]['States'])\n",
    "                    contexts_created[action]['C'+str(j+1)]['States'] = list(contexts_created[action]['C'+str(j+1)]['States'])\n",
    "                    \n",
    "                    \n",
    "                    # Merge contexts\n",
    "                    contexts_created[action]['C'+str(i+1)]['Outcomes'] += contexts_created[action]['C'+str(j+1)]['Outcomes']\n",
    "                    contexts_created[action]['C'+str(i+1)]['States'] += contexts_created[action]['C'+str(j+1)]['States']\n",
    "                    contexts_created[action]['C'+str(i+1)]['Distribution'] = distribution(contexts_created[action]['C'+str(i+1)]['Outcomes'])\n",
    "\n",
    "                    # Replace merged context with the last context\n",
    "                    contexts_created[action]['C'+str(j+1)] = contexts_created[action]['C'+str(n)]\n",
    "                    del contexts_created[action]['C'+str(n)]\n",
    "        \n",
    "    return contexts_created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexts created by the MBRL Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classification = {}\n",
    "list_of_actions = []\n",
    "classification = MBRL_agent(modified_result_list_2, classification, list_of_actions, 0.12, 0.03)\n",
    "\n",
    "# Mapping action → couleur de base (noms de couleurs Seaborn/matplotlib)\n",
    "palette = sns.color_palette(\"Paired\", 6)\n",
    "base_colors = {\n",
    "    'K_1': palette[0],\n",
    "    'K_2': palette[1],\n",
    "    'D_1': palette[2],\n",
    "    'D_2': palette[3],\n",
    "    'L_1': palette[4],\n",
    "    'L_2': palette[5]\n",
    "}\n",
    "\n",
    "# Iterate over actions\n",
    "for action, contexts in classification.items():\n",
    "    context_keys = list(contexts.keys())\n",
    "    n_contexts = len(context_keys)\n",
    "\n",
    "    # Set up grid (max 3 columns)\n",
    "    n_cols = min(3, n_contexts)\n",
    "    n_rows = (n_contexts + n_cols - 1) // n_cols  \n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows), constrained_layout=True)\n",
    "    axes = np.array(axes).flatten()\n",
    "\n",
    "    # Palette plus élégante (ex : cubehelix doux)\n",
    "    palette = sns.color_palette(\"ch:s=.25,rot=-.25\", n_colors=n_contexts)\n",
    "\n",
    "    for ax, context, color in zip(axes, context_keys, palette):\n",
    "        distribution = contexts[context]['Distribution']\n",
    "\n",
    "        x_values = [-1, 0, 1]\n",
    "        y_values = [distribution.get(x, 0) for x in x_values]\n",
    "\n",
    "        # Change x-axis labels to more intuitive labels\n",
    "        ax.bar(x_values, y_values, color=base_colors[action], edgecolor=\"black\", alpha=0.85)\n",
    "        ax.set_title(f\"{context}\", fontsize=11)\n",
    "        #ax.set_xlabel(\"Reward\")\n",
    "        ax.set_ylabel(\"Probability\")\n",
    "        ax.set_xticks([-1, 0, 1])\n",
    "        ax.set_xticklabels([\"Blame\", \"Neutral\", \"Support\"])  # Custom labels\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for ax in axes[n_contexts:]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    # Main title for the figure\n",
    "    fig.suptitle(f\"{action_dict[action]}\", fontsize=16, fontweight=\"bold\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.size': 14,         # taille générale du texte\n",
    "    'axes.titlesize': 18,    # titres des sous-graphes\n",
    "    'axes.labelsize': 16,    # étiquettes des axes\n",
    "    'xtick.labelsize': 13,\n",
    "    'ytick.labelsize': 13,\n",
    "    'legend.fontsize': 14,\n",
    "    'figure.titlesize': 20   # titre global\n",
    "})\n",
    "\n",
    "\n",
    "# Palette contrastée\n",
    "palette = sns.color_palette(\"Paired\", 6)\n",
    "base_colors = {\n",
    "    'K_1': palette[0],\n",
    "    'K_2': palette[1],\n",
    "    'D_1': palette[2],\n",
    "    'D_2': palette[3],\n",
    "    'L_1': palette[4],\n",
    "    'L_2': palette[5]\n",
    "}\n",
    "\n",
    "# États (df_proper → points)\n",
    "state_points = []\n",
    "for action in df_proper[\"Action\"].unique():\n",
    "    action_df = df_proper[df_proper[\"Action\"] == action]\n",
    "    states = natsorted(action_df[\"State\"].unique())\n",
    "    for state in states:\n",
    "        rewards = action_df[action_df[\"State\"] == state][\"Reward\"].tolist()\n",
    "        dist = compute_distribution(rewards)\n",
    "        state_points.append({\n",
    "            'Blame': dist.get(-1, 0),\n",
    "            'Support': dist.get(1, 0),\n",
    "            'Action': action,\n",
    "            'State': state,\n",
    "            'Color': base_colors.get(action, 'gray')\n",
    "        })\n",
    "df_states = pd.DataFrame(state_points)\n",
    "\n",
    "# Clusters (classification → points)\n",
    "cluster_points = []\n",
    "for action, contexts in classification.items():\n",
    "    for context_name, context_data in contexts.items():\n",
    "        dist = context_data[\"Distribution\"]\n",
    "        num_states = len(context_data.get(\"States\", []))\n",
    "        cluster_points.append({\n",
    "            'Blame': dist.get(-1, 0),\n",
    "            'Support': dist.get(1, 0),\n",
    "            'Action': action,\n",
    "            'Context': context_name,\n",
    "            'Color': base_colors.get(action, 'gray'),\n",
    "            'Size': 120 + 80 * num_states \n",
    "        })\n",
    "df_clusters = pd.DataFrame(cluster_points)\n",
    "\n",
    "# Set up subplots: 2 rows x 3 columns\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10), constrained_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot for each action\n",
    "for i, action in enumerate(sorted(df_states[\"Action\"].unique())):\n",
    "    ax = axes[i]\n",
    "    color = base_colors.get(action, 'gray')\n",
    "    label = action_dict.get(action, action)\n",
    "    \n",
    "    # Clusters\n",
    "    subset_clusters = df_clusters[df_clusters[\"Action\"] == action]\n",
    "    ax.scatter(\n",
    "        subset_clusters[\"Blame\"],\n",
    "        subset_clusters[\"Support\"],\n",
    "        s=subset_clusters[\"Size\"],\n",
    "        color=color,\n",
    "        alpha=0.5,\n",
    "        edgecolors=\"black\",\n",
    "        marker='s',\n",
    "        label=\"Clusters\"\n",
    "    )\n",
    "\n",
    "    # États\n",
    "    subset_states = df_states[df_states[\"Action\"] == action]\n",
    "    ax.scatter(\n",
    "        subset_states[\"Blame\"],\n",
    "        subset_states[\"Support\"],\n",
    "        s=60,\n",
    "        color=color,\n",
    "        alpha=1,\n",
    "        edgecolors=\"black\",\n",
    "        label=\"States\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    ax.set_title(f\"{label}\", fontsize=18, fontweight=\"bold\")\n",
    "    ax.set_xlim(-0.05, 1.05)\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    ax.set_xlabel(\"Blame\", fontsize=16)\n",
    "    ax.set_ylabel(\"Support\", fontsize=16)\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "# Remove empty axes if fewer than 6 actions\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Global legend\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexts evolution action by action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ordered_data_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8a53c6cb26c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         classification, snapshots, classification_snapshots = MBRL_agent_3(\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mordered_data_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_of_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0madding_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerging_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ordered_data_1' is not defined"
     ]
    }
   ],
   "source": [
    "# MBRL_agent_3 simulates a model-based RL agent that incrementally clusters contexts\n",
    "# based on state-reward distributions for each action. It tracks the evolution of\n",
    "# context formation and optionally saves snapshots at each step.\n",
    "\n",
    "def MBRL_agent_3(dilemma_list, contexts_created, list_of_actions, adding_threshold, merging_threshold, save_steps=False):\n",
    "    snapshots = []  # Stores the encountered dilemmas at each step (for visualization)\n",
    "    dilemmas_encount = []  # Running list of encountered dilemmas\n",
    "    classification_snapshots = []  # Stores deep copies of the classification state\n",
    "\n",
    "    for dilemma in dilemma_list:\n",
    "        dilemmas_encount.append(dilemma)\n",
    "        reward_distribution = distribution(dilemma['Reward'])  # Compute reward distribution for current state\n",
    "\n",
    "        if dilemma['Action'] in list_of_actions:\n",
    "            context_list = list(contexts_created[dilemma['Action']].keys())\n",
    "\n",
    "            # Check if this exact state has already been seen (and stored in a cluster)\n",
    "            for i in range(len(context_list)):\n",
    "                for j, (state, rewards) in enumerate(contexts_created[dilemma['Action']][f'C{i+1}']['States']):\n",
    "                    if dilemma['State'] == state:\n",
    "                        # If found, update its reward list and remove old rewards from context\n",
    "                        dilemma['Reward'].extend(rewards)\n",
    "                        del_of_a_list(contexts_created[dilemma['Action']][f'C{i+1}']['Outcomes'], rewards)\n",
    "                        del contexts_created[dilemma['Action']][f'C{i+1}']['States'][j]\n",
    "                        # Update distribution after removing old data\n",
    "                        contexts_created[dilemma['Action']][f'C{i+1}']['Distribution'] = distribution(\n",
    "                            contexts_created[dilemma['Action']][f'C{i+1}']['Outcomes'])\n",
    "\n",
    "            # Compute KL divergence between this state and all existing clusters\n",
    "            DKL_context = []\n",
    "            for i in range(len(context_list)):\n",
    "                DKL_context.append(DKL(\n",
    "                    reward_distribution,\n",
    "                    contexts_created[dilemma['Action']][f'C{i+1}']['Distribution']\n",
    "                ))\n",
    "\n",
    "            # Find cluster with minimum KL divergence\n",
    "            min_dkl, context_min_dkl = np.min(DKL_context), np.argmin(DKL_context)\n",
    "\n",
    "            # If divergence too high, create a new context cluster\n",
    "            if min_dkl > adding_threshold:\n",
    "                contexts_created[dilemma['Action']][f'C{len(contexts_created[dilemma[\"Action\"]]) + 1}'] = {\n",
    "                    'Distribution': reward_distribution,\n",
    "                    'Outcomes': dilemma['Reward'],\n",
    "                    'States': [[dilemma['State'], dilemma['Reward']]],\n",
    "                }\n",
    "            else:\n",
    "                # Otherwise, assign to closest cluster and update its stats\n",
    "                selected = contexts_created[dilemma['Action']][f'C{context_min_dkl + 1}']\n",
    "                selected['Outcomes'] = np.concatenate((selected['Outcomes'], dilemma['Reward']))\n",
    "                selected['States'].append([dilemma['State'], dilemma['Reward']])\n",
    "                selected['Distribution'] = distribution(selected['Outcomes'])\n",
    "\n",
    "        else:\n",
    "            # First time this action is seen: create its first context\n",
    "            list_of_actions.append(dilemma['Action'])\n",
    "            contexts_created[dilemma['Action']] = {\n",
    "                'C1': {\n",
    "                    'Distribution': reward_distribution,\n",
    "                    'Outcomes': dilemma['Reward'],\n",
    "                    'States': [[dilemma['State'], dilemma['Reward']]],\n",
    "                }\n",
    "            }\n",
    "\n",
    "        # Attempt to merge similar contexts within each action\n",
    "        for action in list(contexts_created.keys()):\n",
    "            swJS_list = [[], []]  # Stores (i, j) index pairs and their swJS scores\n",
    "            keys = list(contexts_created[action].keys())\n",
    "\n",
    "            if len(keys) > 1:\n",
    "                # Compute swJS divergence between all context pairs\n",
    "                for i in range(len(keys)):\n",
    "                    for j in range(i+1, len(keys)):\n",
    "                        swJS_list[0].append([i, j])\n",
    "                        swJS_list[1].append(\n",
    "                            swJS(\n",
    "                                contexts_created[action][f'C{i+1}']['Distribution'],\n",
    "                                contexts_created[action][f'C{j+1}']['Distribution'],\n",
    "                                len(contexts_created[action][f'C{i+1}']['Outcomes']),\n",
    "                                len(contexts_created[action][f'C{j+1}']['Outcomes'])\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                # Find the pair with smallest swJS divergence\n",
    "                min_swJS, context_min_swJS = np.min(swJS_list[1]), np.argmin(swJS_list[1])\n",
    "\n",
    "                # If below threshold, merge the two contexts\n",
    "                if min_swJS < merging_threshold:\n",
    "                    i, j = swJS_list[0][context_min_swJS]\n",
    "                    n = len(contexts_created[action])\n",
    "\n",
    "                    # Prepare both clusters for merging (convert outcomes to list)\n",
    "                    contexts_created[action][f'C{i+1}']['Outcomes'] = list(contexts_created[action][f'C{i+1}']['Outcomes'])\n",
    "                    contexts_created[action][f'C{j+1}']['Outcomes'] = list(contexts_created[action][f'C{j+1}']['Outcomes'])\n",
    "\n",
    "                    # Merge j into i\n",
    "                    contexts_created[action][f'C{i+1}']['States'] += contexts_created[action][f'C{j+1}']['States']\n",
    "                    contexts_created[action][f'C{i+1}']['Outcomes'] += contexts_created[action][f'C{j+1}']['Outcomes']\n",
    "                    contexts_created[action][f'C{i+1}']['Distribution'] = distribution(contexts_created[action][f'C{i+1}']['Outcomes'])\n",
    "\n",
    "                    # Replace j-th with last context and delete last\n",
    "                    contexts_created[action][f'C{j+1}'] = contexts_created[action][f'C{n}']\n",
    "                    del contexts_created[action][f'C{n}']\n",
    "\n",
    "        # Save a snapshot of current state if requested\n",
    "        if save_steps:\n",
    "            snapshots.append(dilemmas_encount.copy())\n",
    "            classification_snapshot = {a: {k: v.copy() for k, v in contexts_created[a].items()} for a in contexts_created}\n",
    "            classification_snapshots.append(classification_snapshot)\n",
    "\n",
    "    if save_steps:\n",
    "        return contexts_created, snapshots, classification_snapshots\n",
    "    else:\n",
    "        return contexts_created\n",
    "\n",
    "\n",
    "# This function visualizes the evolution of clusters (contexts) for a given action\n",
    "# across saved classification snapshots. Each subplot shows the state and cluster\n",
    "# distributions at a particular step of training.\n",
    "\n",
    "def plot_2(snapshots, classification_snapshots, action_name, i):\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 14,\n",
    "        'axes.titlesize': 16,\n",
    "        'axes.labelsize': 14,\n",
    "        'xtick.labelsize': 12,\n",
    "        'ytick.labelsize': 12,\n",
    "        'legend.fontsize': 12\n",
    "    })\n",
    "\n",
    "    palette = sns.color_palette(\"Paired\", 6)\n",
    "    base_colors = {'K_1': palette[0], 'K_2': palette[1], 'D_1': palette[2],\n",
    "                   'D_2': palette[3], 'L_1': palette[4], 'L_2': palette[5]}\n",
    "\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(18, 15))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for plot_idx, idx in enumerate(range(1, 10)):  # plot steps 2 to 10\n",
    "        if idx >= len(snapshots) or idx >= len(classification_snapshots):\n",
    "            continue\n",
    "\n",
    "        dilemmas_encountered = snapshots[idx]\n",
    "\n",
    "        # Extract all rewards for this action at this step\n",
    "        rows = [\n",
    "            {\n",
    "                \"Action\": entry['Action'],\n",
    "                \"State\": entry['State'],\n",
    "                \"Reward\": reward\n",
    "            }\n",
    "            for entry in dilemmas_encountered if entry['Action'] == action_name\n",
    "            for reward in entry['Reward']\n",
    "        ]\n",
    "        df_proper_func = pd.DataFrame(rows)\n",
    "\n",
    "        # Extract cluster centers for current action at this step\n",
    "        cluster_points = []\n",
    "        if action_name in classification_snapshots[idx]:\n",
    "            for context_name, context_data in classification_snapshots[idx][action_name].items():\n",
    "                dist = context_data[\"Distribution\"]\n",
    "                num_states = len(context_data.get(\"States\", []))\n",
    "                x = dist.get(-1, 0)\n",
    "                y = dist.get(1, 0)\n",
    "                cluster_points.append({\n",
    "                    \"x\": x,\n",
    "                    \"y\": y,\n",
    "                    \"Color\": base_colors.get(action_name, \"gray\"),\n",
    "                    \"Size\": 200 + 80 * num_states\n",
    "                })\n",
    "        df_clusters = pd.DataFrame(cluster_points)\n",
    "\n",
    "        # Compute distribution of individual states\n",
    "        state_points = []\n",
    "        states = natsorted(df_proper_func[\"State\"].unique())\n",
    "        for state in states:\n",
    "            rewards = df_proper_func[df_proper_func[\"State\"] == state][\"Reward\"].tolist()\n",
    "            dist = compute_distribution(rewards)\n",
    "            x = dist.get(-1, 0)\n",
    "            y = dist.get(1, 0)\n",
    "            state_points.append({\n",
    "                'x': x,\n",
    "                'y': y,\n",
    "                'Color': base_colors.get(action_name, 'gray'),\n",
    "            })\n",
    "        df_states = pd.DataFrame(state_points)\n",
    "\n",
    "        # Plot cluster centroids (squares) and states (circles)\n",
    "        ax = axs[plot_idx]\n",
    "        if not df_clusters.empty:\n",
    "            ax.scatter(df_clusters[\"x\"], df_clusters[\"y\"],\n",
    "                       color=df_clusters[\"Color\"], s=df_clusters[\"Size\"],\n",
    "                       alpha=0.6, edgecolors=\"black\", marker='s')\n",
    "        if not df_states.empty:\n",
    "            ax.scatter(df_states[\"x\"], df_states[\"y\"],\n",
    "                       color=df_states[\"Color\"], s=80,\n",
    "                       alpha=1, edgecolors=\"black\")\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title(f\"{action_dict.get(action_name, action_name)} – {idx+1} dilemmas\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Evolution_{action_name}\" + str(i) + \".pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run the clustering and visualization for two datasets\n",
    "actions = ['K_1', 'K_2', 'L_1', 'L_2', 'D_1', 'D_2']\n",
    "for i in range(2):\n",
    "    classification = {}\n",
    "    list_of_actions = []\n",
    "\n",
    "    if i == 0:\n",
    "        classification, snapshots, classification_snapshots = MBRL_agent_3(\n",
    "            ordered_data_1, classification, list_of_actions,\n",
    "            adding_threshold=0.12, merging_threshold=0.03, save_steps=True\n",
    "        )\n",
    "    else:\n",
    "        classification, snapshots, classification_snapshots = MBRL_agent_3(\n",
    "            ordered_data_2, classification, list_of_actions,\n",
    "            adding_threshold=0.12, merging_threshold=0.03, save_steps=True\n",
    "        )\n",
    "\n",
    "    plot_2(snapshots, classification_snapshots, action, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
